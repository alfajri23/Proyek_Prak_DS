print(hasil)
}
bilbulat(100)
bilbulat(100)
bilbulat(100)
}
View(bilbulat)
bilbulat(100)
n = 100+1
100*n/2
bilbulat(100)
bilbulat(100)
bilbulat(1000)
n <- 1000
n <- 1000
n
x <- seq (1, n)
jumlah(x)
x
nilai = 20
sum(nilai)
nilai <- seq(1,20)
sum(nilai)
sum(seq(1,1000))
log (basis = 10, x = 100)
log(y=100,base = 10)
log(100,base=10)
plotSentiments1 <- function(sentiment_dataframe, title)
{
library(ggplot2)
ggplot(sentiment_dataframe, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
ggtitle(title) +
theme(legend.position="right") +
ylab("Number of Tweets") +
xlab("Emotion Categories")
}
library(RCurl)
install.packages("RCurl")
download.file(url=”http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
download.file(url=”http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
download.file(url=”http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem");
write.table(some_txt, file='C:\Drive D\Kuliah\Sem 5\Praktikum Data Science\akhir\uas.txt')
write.table(some_txt, file='C:\ds\uas.txt')
write.table(some_txt, file='C:\ds\uas.txt')
download.file(url=”http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
write.csv(df_himawari, file=’C:\ds\uas.csv’,row.names=FALSE)
write.csv(df_himawari, file='C:\ds\uas.csv’,row.names=FALSE)
write.csv(df_himawari, file="C:\ds\uas.csv",row.names=FALSE)
ACCESS_secret <- "Nh1UzQyh4kDjqFkLIGr5GZtjOz4RV8OB7TZja4CGpnnob"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
Himawari <- searchTwitter(Semeru, n= 100, locale=TRUE)
df_himawari<- do.call("rbind", lapply(Himawari, as.data.frame))
View(df_himawari)
write.csv(df_himawari, file="C:\ds\uas.csv",row.names=FALSE)
library(twitteR)
library(ROAuth)
library(RCurl)
reqURL <- "http://api.twitter.com/oath/request_token"
library(twitteR)
library(ROAuth)
install.packages("ROAuth")
library(RCurl)
library(twitteR)
library(ROAuth)
library(RCurl)
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
CUSTOMER_KEY <- "jZnt1E98V6Sxm3GVIwipRuR9o"
ACCESS_TOKEN <- "2336203339-TFqngPUAPsar2nFjVTUJl1Vv4jvdwzkOEXZLoVe"
CUSTOMER_SECRET <- "JAs48jfgfT0UM0sDCK1fktIhOq85e78AXxAZrQnlZpUWBk0xpM"
ACCESS_secret <- "Nh1UzQyh4kDjqFkLIGr5GZtjOz4RV8OB7TZja4CGpnnob"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
ACCESS_secret <- "Nh1UzQyh4kDjqFkLIGr5GZtjOz4RV8OB7TZja4CGpnnob"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
install.packages(SnowballC)
install.packages(Rstem)
install.packages(twitteR)
install.packages(tm)
install.packages(NLP)
install.packages(SentimentAnalysis)
install.packages(plyr)
install.packages(ggplot2)
install.packages(RColorBrewer)
install.packages(wordcloud)
install.packages(sentiment)
library(SnowballC)
library(Rstem)
library(Rstem)
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
CUSTOMER_KEY <- "jZnt1E98V6Sxm3GVIwipRuR9o"
ACCESS_TOKEN <- "2336203339-TFqngPUAPsar2nFjVTUJl1Vv4jvdwzkOEXZLoVe"
CUSTOMER_SECRET <- "JAs48jfgfT0UM0sDCK1fktIhOq85e78AXxAZrQnlZpUWBk0xpM"
ACCESS_secret <- "Nh1UzQyh4kDjqFkLIGr5GZtjOz4RV8OB7TZja4CGpnnob"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
library(twitteR)
library(ROAuth)
library(RCurl)
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
CUSTOMER_KEY <- "jZnt1E98V6Sxm3GVIwipRuR9o"
ACCESS_TOKEN <- "2336203339-TFqngPUAPsar2nFjVTUJl1Vv4jvdwzkOEXZLoVe"
CUSTOMER_SECRET <- "JAs48jfgfT0UM0sDCK1fktIhOq85e78AXxAZrQnlZpUWBk0xpM"
ACCESS_secret <- "Nh1UzQyh4kDjqFkLIGr5GZtjOz4RV8OB7TZja4CGpnnob"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
Himawari <- searchTwitter(Semeru, n= 100, locale=TRUE)
df_himawari<- do.call("rbind", lapply(Himawari, as.data.frame))
View(df_himawari)
write.csv(df_himawari, file="C:\ds\uas.csv",row.names=FALSE)
Himawari <- searchTwitter('Semeru', n= 100, locale=TRUE)
Himawari <- searchTwitter(`Semeru`, n= 100, locale=TRUE)
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
CUSTOMER_KEY <- "E3XqbcAozKOEKUDuy4skNrygW"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
CUSTOMER_SECRET <- "3hWQXrIYe1ZjBZMLxClVxz0UnOhMDsvfq293ukimhXrEhxQOBQ"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
library(twitteR)
library(ROAuth)
library(RCurl)
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
CUSTOMER_KEY <- "E3XqbcAozKOEKUDuy4skNrygW"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
CUSTOMER_SECRET <- "3hWQXrIYe1ZjBZMLxClVxz0UnOhMDsvfq293ukimhXrEhxQOBQ"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
install.packages("base64enc")
library(base64enc)
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
library(twitteR)
library(ROAuth)
library(RCurl)
library(base64enc)
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
CUSTOMER_KEY <- "E3XqbcAozKOEKUDuy4skNrygW"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
CUSTOMER_SECRET <- "3hWQXrIYe1ZjBZMLxClVxz0UnOhMDsvfq293ukimhXrEhxQOBQ"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
install.packages("openssl")
install.packages("httpuv")
install.packages("httr")
library(twitteR)
library(ROAuth)
library(RCurl)
library(openssl)
library(httpuv)
library(base64enc)
library(httr)
library(twitteR)
library(ROAuth)
library(RCurl)
library(openssl)
library(httpuv)
library(base64enc)
library(httr)
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
CUSTOMER_KEY <- "E3XqbcAozKOEKUDuy4skNrygW"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
CUSTOMER_SECRET <- "3hWQXrIYe1ZjBZMLxClVxz0UnOhMDsvfq293ukimhXrEhxQOBQ"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
CUSTOMER_KEY <- "tadkjHBoBfAP0XFLpTmzt3c9B"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
CUSTOMER_SECRET <- "keRI0AnuEfcAMvq3hV3TNak1MIezdVUjnpxmDa409p2aw5xo3N"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
CUSTOMER_KEY <- "tadkjHBoBfAP0XFLpTmzt3c9B"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
CUSTOMER_SECRET <- "keRI0AnuEfcAMvq3hV3TNak1MIezdVUjnpxmDa409p2aw5xo3N"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
CUSTOMER_KEY <- "tadkjHBoBfAP0XFLpTmzt3c9B"
CUSTOMER_SECRET <- "keRI0AnuEfcAMvq3hV3TNak1MIezdVUjnpxmDa409p2aw5xo3N"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
library(twitteR)
library(ROAuth)
library(RCurl)
library(openssl)
library(httpuv)
library(base64enc)
library(httr)
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
CUSTOMER_KEY <- "tadkjHBoBfAP0XFLpTmzt3c9B"
CUSTOMER_SECRET <- "keRI0AnuEfcAMvq3hV3TNak1MIezdVUjnpxmDa409p2aw5xo3N"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
setup_twitter_oauth(CUSTOMER_KEY, CUSTOMER_SECRET, ACCESS_TOKEN, ACCESS_secret)
library(twitteR)
library(ROAuth)
library(RCurl)
library(openssl)
library(httpuv)
library(base64enc)
library(httr)
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
reqURL <- "http://api.twitter.com/oath/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
CUSTOMER_KEY <- "tadkjHBoBfAP0XFLpTmzt3c9B"
CUSTOMER_SECRET <- "keRI0AnuEfcAMvq3hV3TNak1MIezdVUjnpxmDa409p2aw5xo3N"
ACCESS_TOKEN <- "2336203339-5itMQYzKKn9muEmU8wu3AB1Nk6ofV1smZNp1VBJ"
ACCESS_secret <- "zd8cpcOe5823pgvTaPcEXduuKkiJZKP3t2ngEhTMLu19d"
my_oauth <- OAuthFactory$new(consumerKey = CUSTOMER_KEY,
consumerSecret = CUSTOMER_SECRET,
requestURL = reqURL,
accessURL = accessURL,
authURL = authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package =
"RCurl"))
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package ="RCurl"))
load("C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Data Science/.RData")
df<-hotel_reviews
set.seed(20)
library(tidyverse)
library(ggtech)
library(tidytext)
library(widyr)
library(stm)
library(e1071) #untuk naive bayes
library(caret) #untuk klasifikasi data
library(syuzhet)
library(tm)
library(dplyr)
library(wordcloud)
library(cluster)    # clustering algorithms
library(factoextra)
hotel_reviews <- read_csv("./tripadvisor_hotel_reviews.csv", show_col_types=FALSE) %>%
rename_with(str_to_lower)
hotel_words <- hotel_reviews %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]")) %>%
count( word, sort=TRUE)
hotel_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by = "word") %>%
filter(!word %in% c("n't", "hotel")) %>%
count(hotel_id, rating, word) %>%
group_by(word)
review_matrix <- hotel_review_words %>%
filter(n >= 5) %>%
cast_sparse(hotel_id, word, n)
topic_model_6 <- stm(review_matrix,
K=6,
verbose = TRUE,
init.type = "Spectral",
emtol = 5e-5)
user_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]"),
!word %in% c("n't", 'hotel')) %>%
count(hotel_id,rating, word, sort=TRUE)
by_word <- user_review_words %>%
group_by(word) %>%
summarize(avg_rating = mean(rating),
nb_reviews = n()) %>%
arrange(desc(nb_reviews)) %>%
filter(nb_reviews >= 500) %>%
arrange(desc(avg_rating))
df<-hotel_reviews
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
corpus<-Corpus(VectorSource(df$review))
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
length <- nrow(df)
length_train <- as.integer(length*0.75)
df.train<-df[1:length_train,]
df.test<-df[length_train:length,]
corpus.clean.train<-corpus.clean[1:15368]
corpus.clean.test<-corpus.clean[15368:20491]
indexed <- by_word
indexed <- indexed %>% column_to_rownames(., var = "word")
library(ggtech)
library(tidytext)
library(ggtech)
library(tidytext)
library(ggtech)
install.packages("ggtech")
library(tidytext)
library(tidyverse)
library(ggtech)
library(tidytext)
library(widyr)
library(stm)
library(e1071) #untuk naive bayes
library(caret) #untuk klasifikasi data
library(syuzhet)
library(tm)
library(dplyr)
library(wordcloud)
library(cluster)    # clustering algorithms
library(factoextra)
hotel_reviews <- read_csv("./tripadvisor_hotel_reviews.csv", show_col_types=FALSE) %>%
rename_with(str_to_lower)
hotel_words <- hotel_reviews %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]")) %>%
count( word, sort=TRUE)
hotel_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by = "word") %>%
filter(!word %in% c("n't", "hotel")) %>%
count(hotel_id, rating, word) %>%
group_by(word)
review_matrix <- hotel_review_words %>%
filter(n >= 5) %>%
cast_sparse(hotel_id, word, n)
topic_model_6 <- stm(review_matrix,
K=6,
verbose = TRUE,
init.type = "Spectral",
emtol = 5e-5)
user_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]"),
!word %in% c("n't", 'hotel')) %>%
count(hotel_id,rating, word, sort=TRUE)
by_word <- user_review_words %>%
group_by(word) %>%
summarize(avg_rating = mean(rating),
nb_reviews = n()) %>%
arrange(desc(nb_reviews)) %>%
filter(nb_reviews >= 500) %>%
arrange(desc(avg_rating))
df<-hotel_reviews
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
corpus<-Corpus(VectorSource(df$review))
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
length <- nrow(df)
length_train <- as.integer(length*0.75)
df.train<-df[1:length_train,]
df.test<-df[length_train:length,]
corpus.clean.train<-corpus.clean[1:15368]
corpus.clean.test<-corpus.clean[15368:20491]
indexed <- by_word
indexed <- indexed %>% column_to_rownames(., var = "word")
hotel_reviews <- read_csv("C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Annas/ProjectAkhirDS-master/tripadvisor_hotel_reviews.csv", show_col_types=FALSE) %>%
rename_with(str_to_lower)
hotel_reviews <- read_csv("C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Annas/ProjectAkhirDS-master/tripadvisor_hotel_reviews.csv", show_col_types=FALSE) %>%
rename_with(str_to_lower)
hotel_words <- hotel_reviews %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]")) %>%
count( word, sort=TRUE)
hotel_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by = "word") %>%
filter(!word %in% c("n't", "hotel")) %>%
count(hotel_id, rating, word) %>%
group_by(word)
review_matrix <- hotel_review_words %>%
filter(n >= 5) %>%
cast_sparse(hotel_id, word, n)
topic_model_6 <- stm(review_matrix,
K=6,
verbose = TRUE,
init.type = "Spectral",
emtol = 5e-5)
user_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]"),
!word %in% c("n't", 'hotel')) %>%
count(hotel_id,rating, word, sort=TRUE)
by_word <- user_review_words %>%
group_by(word) %>%
summarize(avg_rating = mean(rating),
nb_reviews = n()) %>%
arrange(desc(nb_reviews)) %>%
filter(nb_reviews >= 500) %>%
arrange(desc(avg_rating))
df<-hotel_reviews
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
corpus<-Corpus(VectorSource(df$review))
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
length <- nrow(df)
length_train <- as.integer(length*0.75)
df.train<-df[1:length_train,]
df.test<-df[length_train:length,]
corpus.clean.train<-corpus.clean[1:15368]
corpus.clean.test<-corpus.clean[15368:20491]
indexed <- by_word
indexed <- indexed %>% column_to_rownames(., var = "word")
shiny::runApp('C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Annas/ProjectAkhirDS-master')
install.packages("ggtech")
runApp('C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Annas/ProjectAkhirDS-master')
runApp('C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Annas/ProjectAkhirDS-master')
library(tidyverse)
#library(ggtech)
library(tidytext)
library(widyr)
library(stm)
library(e1071) #untuk naive bayes
library(caret) #untuk klasifikasi data
library(syuzhet)
library(tm)
library(dplyr)
library(wordcloud)
library(cluster)    # clustering algorithms
library(factoextra)
hotel_reviews <- read_csv("C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Annas/ProjectAkhirDS-master/tripadvisor_hotel_reviews.csv", show_col_types=FALSE) %>%
rename_with(str_to_lower)
hotel_words <- hotel_reviews %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]")) %>%
count( word, sort=TRUE)
hotel_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by = "word") %>%
filter(!word %in% c("n't", "hotel")) %>%
count(hotel_id, rating, word) %>%
group_by(word)
review_matrix <- hotel_review_words %>%
filter(n >= 5) %>%
cast_sparse(hotel_id, word, n)
topic_model_6 <- stm(review_matrix,
K=6,
verbose = TRUE,
init.type = "Spectral",
emtol = 5e-5)
user_review_words <- hotel_reviews %>%
mutate(hotel_id = row_number()) %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by="word") %>%
filter(str_detect(word, "[a-z]"),
!word %in% c("n't", 'hotel')) %>%
count(hotel_id,rating, word, sort=TRUE)
by_word <- user_review_words %>%
group_by(word) %>%
summarize(avg_rating = mean(rating),
nb_reviews = n()) %>%
arrange(desc(nb_reviews)) %>%
filter(nb_reviews >= 500) %>%
arrange(desc(avg_rating))
df<-hotel_reviews
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
corpus<-Corpus(VectorSource(df$review))
corpus.clean<-corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
length <- nrow(df)
length_train <- as.integer(length*0.75)
df.train<-df[1:length_train,]
df.test<-df[length_train:length,]
corpus.clean.train<-corpus.clean[1:15368]
corpus.clean.test<-corpus.clean[15368:20491]
indexed <- by_word
indexed <- indexed %>% column_to_rownames(., var = "word")
runApp('C:/Drive D/Kuliah/Sem 5/Praktikum Data Science/akhir/Annas/ProjectAkhirDS-master')
